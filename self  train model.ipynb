{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69be4cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "path_ = find_dotenv()\n",
    "load_dotenv(path_)\n",
    "\n",
    "KEY_ID = os.getenv(\"KEY_ID\")\n",
    "ACCESS_KEY =os.getenv(\"ACCESS_KEY\")\n",
    "REGION = os.getenv(\"REGION\")\n",
    "\n",
    "\n",
    "\n",
    "S3 = boto3.resource(\n",
    "    's3',\n",
    "    aws_access_key_id=KEY_ID,\n",
    "    aws_secret_access_key=ACCESS_KEY,\n",
    "    region_name=REGION\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19dda0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "bckt_nme = \"imdb-dataset-11.8\"  \n",
    "file_name = \"IMDB Dataset.csv\"\n",
    "PATH = r\"M:\\\\MACHINE LEARNING\\\\github\\\\sentiment analysis\"\n",
    "\n",
    "os.makedirs(PATH, exist_ok=True)\n",
    "\n",
    "if os.path.exists(os.path.join(PATH, file_name)):\n",
    "    print(\"File already exists\")\n",
    "else:\n",
    "    S3.Bucket(bckt_nme).download_file(file_name, os.path.join(PATH, file_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5044c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: 50000\n",
      "duplicated : 418\n",
      "after adding total duplicates : 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "df =pd.read_csv(os.path.join(PATH,file_name))\n",
    "print(f\"total sample : {df.shape[0]}\")\n",
    "print(f\"duplicated : {df.duplicated().sum()}\")\n",
    "\n",
    "df.drop_duplicates(inplace=True)\n",
    "print(f\"after adding total duplicates : {df.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84802dc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49582,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "df['sentiment'] = encoder.fit_transform(df['sentiment'])\n",
    "df['sentiment'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6834d87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Hp/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "def make_lower(text): \n",
    "    return text.lower()\n",
    "from lxml import html\n",
    "\n",
    "def remove_tags(text): \n",
    "    doc = html.fromstring(text) # separate the tags \n",
    "    return doc.text_content() # collect the text and shows it \n",
    "import re\n",
    "\n",
    "def remove_url(text): \n",
    "    patters = re.compile(r'https?://\\S+|www.\\S+')\n",
    "    return patters.sub(\" \",text)\n",
    "import string\n",
    "puns = string.punctuation\n",
    "\n",
    "def remove_puns(text): \n",
    "    return text.translate(str.maketrans('','',puns))\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        #isinstance(text, str) → True if text is a string\n",
    "        # not text.strip() checks if the text is empty or only whitespace.\n",
    "        # jodi string or space kno tai na hoi thle \"\" return korbe \n",
    "        return \"\"\n",
    "    words = text.split()\n",
    "    filtered = [word for word in words if word.lower() not in stop_words]\n",
    "    return \" \".join(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f991351a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "for pkg in ['punkt', 'punkt_tab']:\n",
    "    try:\n",
    "        nltk.data.find(f'tokenizers/{pkg}')\n",
    "    except LookupError:\n",
    "        nltk.download(pkg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e2d2f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import spacy\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "\n",
    "\n",
    "\n",
    "def preprocessor(text): \n",
    "    text = make_lower(text)\n",
    "    text = remove_tags(text)\n",
    "    text = remove_url(text)\n",
    "    text = remove_puns(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = word_tokenize(text)\n",
    "\n",
    "    return (text)\n",
    "\n",
    "df['clean_text']=df['review'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "98f1ac6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models  import Word2Vec\n",
    "\n",
    "words =df['clean_text'].tolist() # its mainly the nested list of clean text \n",
    "    # [ ['i','love','this','movie'],['too','bad'], ... ...]\n",
    "\n",
    "\n",
    "if os.path.exists(r\"M:\\MACHINE LEARNING\\github\\sentiment analysis\\word2vec_model.model\"):\n",
    "    word2vec_model = Word2Vec.load(\"M:\\MACHINE LEARNING\\github\\sentiment analysis\\word2vec_model.model\")\n",
    "    print(\"model loaded\")\n",
    "else: \n",
    "    word2vec_model= Word2Vec(\n",
    "        vector_size=100,\n",
    "        window=5,\n",
    "        min_count=2,\n",
    "        sg=1\n",
    "    )\n",
    "                                                    # this model just find out the relation of the words \n",
    "    word2vec_model.build_vocab(words)                        # and convert words into a 100d vector \n",
    "\n",
    "    word2vec_model.train(\n",
    "        words,\n",
    "        total_examples=df.shape[0],\n",
    "        epochs=2\n",
    "    )\n",
    "    word2vec_model.save(\"word2vec_model.model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "58aa2bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=50000,oov_token =\"<OOV>\")\n",
    "# By default, it will:\n",
    "# Convert text to lowercase,\n",
    "# Split on spaces,\n",
    "# Remove punctuation,\n",
    "# Count word frequencies (when you fit it).\n",
    "\n",
    "tokenizer.fit_on_texts([\" \".join(token) for token in words]) # convert all the words into number value \n",
    "\n",
    "# Save tokenizer to JSON file\n",
    "tokenizer_json = tokenizer.to_json()\n",
    "\n",
    "with open('tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(tokenizer_json)\n",
    "\n",
    "\n",
    "seqnce = tokenizer.texts_to_sequences(words) # sentence replace with their word's number value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "570a1980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sb gula review k ebr same size kora hoche karon model input gula sb same size hoite hoi \n",
    "\n",
    "x = pad_sequences(\n",
    "    seqnce,\n",
    "    maxlen=200, # maximum size of each review \n",
    "    padding = 'pre', # 200 thke kom hole last e 0 add kore lenth 200 kora hbe\n",
    "    truncating = 'post' # 200 thke beshi hoile last thke token delte kore lenth 200 kora hbe\n",
    ")\n",
    "\n",
    "y = df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba12f4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "m:\\MACHINE LEARNING\\github\\sentiment analysis\\myenv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "from keras.layers import *\n",
    "from keras.models import Sequential\n",
    "\n",
    "import numpy as np\n",
    "vocab_size = min(50000,len(tokenizer.word_index)+1) # total number of token \n",
    "embedding_dim = 100 # each word is 100d vector \n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= vocab_size:      # skip indices outside the embedding matrix\n",
    "        continue\n",
    "    if word in word2vec_model.wv:\n",
    "        embedding_matrix[i] = word2vec_model.wv[word]\n",
    "\n",
    "embedding_layer = tf.keras.layers.Embedding(\n",
    "    input_dim=vocab_size,\n",
    "    output_dim=100,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=200,\n",
    "    trainable=False  # freeze if you don't want to fine-tune\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6e23dc54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,000,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │     \u001b[38;5;34m5,000,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,000,000</span> (19.07 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,000,000\u001b[0m (19.07 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,000,000</span> (19.07 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m5,000,000\u001b[0m (19.07 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Sequential name=sequential_1, built=False>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "\n",
    "def create_model(embedding_layer):\n",
    "    sentiment_model = Sequential([\n",
    "        embedding_layer,\n",
    "        LSTM(128, return_sequences=True),\n",
    "        LSTM(128, return_sequences = False),\n",
    "        Dropout(0.4),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    sentiment_model.compile(loss='binary_crossentropy', \n",
    "                            optimizer='adam',\n",
    "                            metrics=['accuracy'])\n",
    "    sentiment_model.summary()\n",
    "    return sentiment_model\n",
    "\n",
    "\n",
    "create_model(embedding_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d60864e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "258b5d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# Folder where you want to save checkpoints\n",
    "PATH = r\"M:\\MACHINE LEARNING\\github\\sentiment analysis\"\n",
    "os.makedirs(PATH, exist_ok=True)\n",
    "\n",
    "checkpoint_dir = r\"M:\\MACHINE LEARNING\\github\\sentiment analysis\\checkpoints\"\n",
    "# Filepath must end with .weights.h5 when save_weights_only=True\n",
    "checkpoint_path = os.path.join(checkpoint_dir, \"cp-{epoch:04d}.weights.h5\")\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    save_weights_only=True,\n",
    "    verbose=1,\n",
    "    save_freq='epoch'  # Save after every epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52166c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment_model.fit(x_train, \n",
    "#                     y_train, \n",
    "#                     epochs=10, \n",
    "#                     batch_size=128, \n",
    "#                     validation_split=0.2,\n",
    "#                     callbacks=[cp_callback]\n",
    "#                     )\n",
    "# sentiment_model.save(\"new_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e932d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import h5py\n",
    "h5py.is_hdf5(r\"M:\\MACHINE LEARNING\\github\\sentiment analysis\\new_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a8e790",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = keras.models.load_model(r\"M:\\MACHINE LEARNING\\github\\sentiment analysis\\new_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac2cba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 127ms/step - accuracy: 0.8894 - loss: 0.2822\n",
      "Test Loss: 28.22%\n",
      "Test Accuracy: 88.94%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "\n",
    "loss, acc = test_model.evaluate(x_test, y_test)\n",
    "print(f\"Test Loss: {loss * 100:.2f}%\")\n",
    "print(f\"Test Accuracy: {acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b7dc4936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238ms/step\n",
      "Negative Review \n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "import json\n",
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "\n",
    "# Example sentence\n",
    "text = (\"great movie\")\n",
    "\n",
    "def test_case(text):\n",
    "    # Convert it to sequence\n",
    "    with open('tokenizer.json', 'r', encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "        tokenizer = tokenizer_from_json(data)\n",
    "\n",
    "    seq = tokenizer.texts_to_sequences([text]) \n",
    "    # tokenizer object will create unique word list and assign them a unique value .\n",
    "    #  texts_to_sequences this class will replace the word with their unique value . \n",
    "\n",
    "    # Pad to the same length as training data\n",
    "    padded = pad_sequences(seq, maxlen=200, padding='pre') \n",
    "    # makes the input same size as training data input \n",
    "\n",
    "    # Predict\n",
    "    pred = test_model.predict(padded)\n",
    "    return pred\n",
    "\n",
    "test_case(text)\n",
    "\n",
    "if pred > 0.5:\n",
    "    print(\"Positive Review \")\n",
    "else:\n",
    "    print(\"Negative Review \")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7c5bc3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5e0510",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
