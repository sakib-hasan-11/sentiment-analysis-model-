{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95f099a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From m:\\MACHINE LEARNING\\github\\sentiment analysis\\myenv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer , TFAutoModelForSequenceClassification\n",
    "import boto3 \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lxml \n",
    "import os\n",
    "import re \n",
    "import string\n",
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ab4715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boto3 : 1.40.54\n",
      "pandas: 2.3.3\n",
      "numpy : 1.26.4\n",
      "tensorflow : 2.15.1\n"
     ]
    }
   ],
   "source": [
    "print(f\"boto3 : {boto3.__version__}\")\n",
    "print(f\"pandas: {pd.__version__}\")\n",
    "print(f\"numpy : {np.__version__}\")\n",
    "print(f\"tensorflow : {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2285e5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "path = find_dotenv()# this will find the .env file \n",
    "load_dotenv(path) # read the data from .env file \n",
    "\n",
    "# Access environment variables\n",
    "KEY_ID = os.getenv(\"KEY_ID\")\n",
    "ACCESS_KEY = os.getenv(\"ACCESS_KEY\")\n",
    "REGION = os.getenv(\"REGION\")\n",
    "\n",
    "s3_object = boto3.resource( \n",
    "    service_name = \"s3\",\n",
    "    region_name = REGION,\n",
    "    aws_access_key_id = KEY_ID,\n",
    "    aws_secret_access_key = ACCESS_KEY\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21c19000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dont exicute\n"
     ]
    }
   ],
   "source": [
    "exicute = False \n",
    "\n",
    "nw_bckt_nme = \"imdb-dataset-11.8\" # new bucket name \n",
    "file_path = r\"M:\\\\MACHINE LEARNING\\\\datasets\\\\IMDB Dataset.csv\" # local file path \n",
    "s3_file_name = \"IMDB Dataset.csv\" # this will be remote file name \n",
    "\n",
    "if exicute == True: \n",
    "\n",
    "    s3_object.create_bucket(\n",
    "        Bucket = nw_bckt_nme, # unique bucket name \n",
    "        CreateBucketConfiguration = {\"LocationConstraint\": REGION} # fixed the region \n",
    "    )\n",
    "\n",
    "    file_path = r\"M:\\\\MACHINE LEARNING\\\\datasets\\\\IMDB Dataset.csv\" # local path\n",
    "    s3_file_name = \"IMDB Dataset.csv\" # remote file name \n",
    "\n",
    "    s3_object.Bucket(nw_bckt_nme).upload_file(file_path,s3_file_name)\n",
    "\n",
    "else : \n",
    "    print(\"dont exicute\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf77547e",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_file_name = \"IMDB Dataset.csv\"\n",
    "nw_bckt_nme = \"imdb-dataset-11.8\"\n",
    "\n",
    "file_obj = s3_object.Bucket(nw_bckt_nme).Object(s3_file_name).get()\n",
    "df = pd.read_csv(file_obj['Body'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8cdeff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review       0\n",
      "sentiment    0\n",
      "dtype: int64\n",
      "dups: 418\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())\n",
    "print(f\"dups: {df.duplicated().sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c64586d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dups: 0\n"
     ]
    }
   ],
   "source": [
    "df.drop_duplicates(inplace=True)\n",
    "print(f\"dups: {df.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3dca5f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "df['sentiment'] = encoder.fit_transform(df['sentiment'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b40b33b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  One of the other reviewers has mentioned that ...          1\n",
       "1  A wonderful little production. <br /><br />The...          1\n",
       "2  I thought this was a wonderful way to spend ti...          1\n",
       "3  Basically there's a family where a little boy ...          0\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...          1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "964d7b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from lxml import html\n",
    "\n",
    "\n",
    "def clean_text (text): \n",
    "    text.lower()\n",
    "\n",
    "    patters = re.compile(r'https?://\\S+|www.\\S+')\n",
    "    patters.sub(\" \",text)\n",
    "    \n",
    "    puns = string.punctuation\n",
    "    text.translate(str.maketrans('','',puns))\n",
    "\n",
    "    doc = html.fromstring(text) \n",
    "    return doc.text_content()\n",
    "\n",
    "\n",
    "df['reviews'] = df['review'].apply(clean_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad1b760c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = df['sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['reviews'],y,test_size = 0.2,random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44d09642",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trnsformr = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(trnsformr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5996778",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_string_data = X_train.astype(str).tolist()\n",
    "test_string_data = X_test.astype(str).tolist()\n",
    "# transformer takes the string list not a pandas series . \n",
    "\n",
    "train_tokens = tokenizer(\n",
    "    train_string_data,\n",
    "    truncation = True,\n",
    "    padding = True\n",
    "    )\n",
    "\n",
    "# transformer takes the string list not a pandas series .\n",
    "test_tokens = tokenizer(\n",
    "    test_string_data,\n",
    "    truncation = True,\n",
    "    padding = True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4baaff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert token into tensor for training \n",
    "\n",
    "\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_tokens),\n",
    "    y_train.values\n",
    ")).shuffle(1000).batch(16)\n",
    "\n",
    "test_data = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(test_tokens),\n",
    "    y_test.values\n",
    ")).batch(16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "119b0b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 76dbce33-82dc-4e2c-a72f-f6676d9717be)')' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From m:\\MACHINE LEARNING\\github\\sentiment analysis\\myenv\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n",
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# fine tuning transformer for sentiment analysis. \n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "trnsformr = \"bert-base-uncased\"\n",
    "output_label = 2 # possitive and negtive \n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    trnsformr, # define transformer used for train\n",
    "    from_pt = True, #convert the weights into tensorflow format to use \n",
    "    num_labels= output_label # total outputs \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4e994b",
   "metadata": {},
   "source": [
    "training arguments will work if we use transformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a590a290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./results\",\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     save_strategy=\"epoch\",\n",
    "#     logging_dir=\"./logs\",\n",
    "#     learning_rate=2e-5,\n",
    "#     per_device_train_batch_size=8,\n",
    "#     num_train_epochs=3,\n",
    "#     weight_decay=0.01,\n",
    "# )\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_data,\n",
    "#     eval_dataset=test_data\n",
    "#     )\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6acefca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202ad0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#history = model.fit(\n",
    "#     train_data,\n",
    "#     validation_data=test_data,\n",
    "#     epochs=1\n",
    "# )\n",
    "\n",
    "model.save_pretrained(\"transformer_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d9e1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = [\"I love this movie!\", \"It was terrible.\"]\n",
    "# inputs = tokenizer(text, return_tensors=\"tf\", padding=True, truncation=True)\n",
    "# preds = model(inputs)\n",
    "# probs = tf.nn.softmax(preds.logits, axis=-1)\n",
    "# print(probs.numpy())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
